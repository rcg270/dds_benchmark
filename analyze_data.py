"""
This script analyzes the monitor_output.log files generated by the DDS benchmark
run this script after running run_benchmark.py
afterwards you can plot the results with plot_results.py
"""
import os
import re
import pandas as pd


def analyze_monitor_logs(log_base_dir="logs"):
    """
    Analyzes monitor_output.log files to extract final benchmark statistics
    Returns a tuple containing pandas.DataFrame object
    """
    all_final_results = []

    # Regex to parse the final summary line
    final_line_pattern = re.compile(
        r"\[Final\] Samples: (\d+) \| Avg Latency: ([-+]?\d+\.\d+) ms \| "
        r"Min Latency: ([-+]?\d+\.\d+) ms \| Max Latency: ([-+]?\d+\.\d+) ms \| "
        r"Std Latency: (\d+\.\d+) ms \| Throughput: (\d+\.\d+) msg/s \| "
        r"Bandwidth: (\d+\.\d+) MB/s \| Duration: (\d+\.\d+) s"
    )

    print(f"Starting analysis from: {os.path.abspath(log_base_dir)}")

    for rmw_impl in os.listdir(log_base_dir):
        rmw_impl_path = os.path.join(log_base_dir, rmw_impl)
        if not os.path.isdir(rmw_impl_path):
            continue

        print(f"Processing RMW: {rmw_impl}")
        for topic_dir in os.listdir(rmw_impl_path):
            topic_path = os.path.join(rmw_impl_path, topic_dir)
            if not os.path.isdir(topic_path):
                continue

            print(f"  Processing Topic: {topic_dir}")
            for timestamp_dir in os.listdir(topic_path):
                timestamp_path = os.path.join(topic_path, timestamp_dir)
                if not os.path.isdir(timestamp_path):
                    continue

                log_file_path = os.path.join(
                    timestamp_path, "monitor_output.log"
                )
                if not os.path.exists(log_file_path):
                    print(
                        f"    Warning: monitor_output.log not found in {timestamp_path}"
                    )
                    continue

                with open(log_file_path, "r") as f:
                    log_content = (
                        f.readlines()
                    )
                    for _, line in enumerate(log_content):
                        # Try to match the final summary line
                        final_match = final_line_pattern.search(line)
                        if final_match:
                            data = final_match.groups()
                            result = {
                                "RMW_Implementation": rmw_impl,
                                "Topic": topic_dir.replace("_", "/"),
                                "Timestamp_Run": timestamp_dir,  # Unique identifier for this run
                                "Samples": int(data[0]),
                                "Avg_Latency_ms": float(data[1]),
                                "Min_Latency_ms": float(data[2]),
                                "Max_Latency_ms": float(data[3]),
                                "Std_Latency_ms": float(data[4]),
                                "Throughput_msg_s": float(data[5]),
                                "Bandwidth_MB_s": float(data[6]),
                                "Duration_s": float(data[7]),
                            }
                            all_final_results.append(result)

    # Create DataFrames
    result_df = (
        pd.DataFrame(all_final_results) if all_final_results else pd.DataFrame()
    )

    return result_df


if __name__ == "__main__":
    """
    Adjust log base dir accordingly to where your logs are stored.
    Standard structure is:
    ./logs
    """
    log_base_dir = "./logs"

    results = analyze_monitor_logs(log_base_dir=log_base_dir)

    if not results.empty:
        print("\n--- Benchmark Results (Summary) ---")
        print(results.to_string())
        results.to_csv("benchmark_summary.csv", index=False)
        print("\nAggregated results saved to benchmark_summary.csv")
    else:
        print("No aggregated data found to display or save.")
